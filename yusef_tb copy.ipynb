{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc71d8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"args = {}\\nargs['image_size'] = (256, 256)\\nargs['patch_size'] = (2, 2)\\nargs['dims'] = [96, 120, 144]\\n#the last channel argument determines the amount of output channels. SET = 1\\nargs['channels'] = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 1]\\nargs['expansion'] = 1\\nargs['kernel_size'] = 3\\nargs['depths'] = (2, 4, 3)\\nargs['in_channels'] = 1\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from data import HFFH_ViT_Data\n",
    "from loss import Loss\n",
    "from model import  HFFH_ViT\n",
    "from trainer import Trainer\n",
    "from saved import Saver\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "im = torch.randn((4, 1, 8, 8))\n",
    "print(im.shape)\n",
    "\n",
    "'''args = {}\n",
    "args['image_size'] = (256, 256)\n",
    "args['patch_size'] = (2, 2)\n",
    "args['dims'] = [96, 120, 144]\n",
    "#the last channel argument determines the amount of output channels. SET = 1\n",
    "args['channels'] = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 1]\n",
    "args['expansion'] = 1\n",
    "args['kernel_size'] = 3\n",
    "args['depths'] = (2, 4, 3)\n",
    "args['in_channels'] = 1'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39bf5092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro T1000\n",
      "Created HFFH_ViT Model\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "args = {}\n",
    "# model\n",
    "args['precision'] = \"single\"\n",
    "args['device'] = torch.device(\"cuda:0\")\n",
    "args['image_size'] = (256, 256)\n",
    "args['patch_size'] = (2, 2)\n",
    "args['dims'] = [16, 16, 16]\n",
    "#args['dims'] = [96, 120, 144]\n",
    "#the last channel argument determines the amount of output channels. SET = 1\n",
    "args['channels'] = [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1]\n",
    "#args['channels'] = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 1]\n",
    "args['expansion'] = 1\n",
    "args['kernel_size'] = 3\n",
    "args['depths'] = (2, 4, 3)\n",
    "args['in_channels'] = 1\n",
    "\n",
    "# loss function\n",
    "args['loss'] = \"1*L1\"  # loss functions separated by '+', each loss function has [weight]*[loss_type]\n",
    "\n",
    "# trainer\n",
    "args['optimizer'] = \"ADAM\"\n",
    "args['lr'] = 1e-4\n",
    "args['weight_decay'] = 0\n",
    "args['decay'] = \"10-20-30-50-75-100-150-200\"   # Decay milestones\n",
    "args['gamma'] = 0.5     # Decay factor at each milestone\n",
    "args['betas'] = (0.9, 0.999)\n",
    "args['epsilon'] = 1e-8\n",
    "args['print_every'] = 0 # 0 = never\n",
    "args['epochs'] = 1e8\n",
    "args['batch_size'] = 32\n",
    "\n",
    "model = HFFH_ViT(args)\n",
    "im = model(im)\n",
    "#print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5fe2d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded radarImagesFFH from hffh_ViT_solid_w_points_2048_training\n",
      "loaded radarImagesFFH from hffh_ViT_solid_w_points_2048_training_v2\n",
      "loaded idealImages from hffh_ViT_solid_w_points_2048_training\n",
      "loaded idealImages from hffh_ViT_solid_w_points_2048_training_v2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "args = {}\n",
    "args['device'] = torch.device(\"cuda:0\")\n",
    "d = HFFH_ViT_Data(args)\n",
    "\n",
    "d.create_train_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa8887be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.cuda.FloatTensor'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.dataset_train.tensors[0].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d4bcaa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './saved/hffh_ViT_v1.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\WISLAB\\OneDrive\\Josiah Academics\\PhD\\Repositories\\hybrid-freehand-imaging-ViT\\yusef_tb copy.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/WISLAB/OneDrive/Josiah%20Academics/PhD/Repositories/hybrid-freehand-imaging-ViT/yusef_tb%20copy.ipynb#ch0000004?line=0'>1</a>\u001b[0m s \u001b[39m=\u001b[39m Saver()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/WISLAB/OneDrive/Josiah%20Academics/PhD/Repositories/hybrid-freehand-imaging-ViT/yusef_tb%20copy.ipynb#ch0000004?line=1'>2</a>\u001b[0m args, m, l, t \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39;49mLoad(d, HFFH_ViT, Loss, Trainer, \u001b[39m\"\u001b[39;49m\u001b[39m./saved/hffh_ViT_v1.tar\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\WISLAB\\OneDrive\\Josiah Academics\\PhD\\Repositories\\hybrid-freehand-imaging-ViT\\saved\\__init__.py:23\u001b[0m, in \u001b[0;36mSaver.Load\u001b[1;34m(self, data, ModelClass, LossClass, TrainerClass, PATH)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/WISLAB/OneDrive/Josiah%20Academics/PhD/Repositories/hybrid-freehand-imaging-ViT/saved/__init__.py?line=19'>20</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/WISLAB/OneDrive/Josiah%20Academics/PhD/Repositories/hybrid-freehand-imaging-ViT/saved/__init__.py?line=20'>21</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/WISLAB/OneDrive/Josiah%20Academics/PhD/Repositories/hybrid-freehand-imaging-ViT/saved/__init__.py?line=22'>23</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(PATH)\n\u001b[0;32m     <a href='file:///c%3A/Users/WISLAB/OneDrive/Josiah%20Academics/PhD/Repositories/hybrid-freehand-imaging-ViT/saved/__init__.py?line=24'>25</a>\u001b[0m args \u001b[39m=\u001b[39m checkpoint[\u001b[39m'\u001b[39m\u001b[39margs\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='file:///c%3A/Users/WISLAB/OneDrive/Josiah%20Academics/PhD/Repositories/hybrid-freehand-imaging-ViT/saved/__init__.py?line=25'>26</a>\u001b[0m model \u001b[39m=\u001b[39m ModelClass(args)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=590'>591</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=591'>592</a>\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=593'>594</a>\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=594'>595</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=595'>596</a>\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=596'>597</a>\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=597'>598</a>\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=598'>599</a>\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=227'>228</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=228'>229</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=229'>230</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=230'>231</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=231'>232</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=209'>210</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> <a href='file:///~/Anaconda3/lib/site-packages/torch/serialization.py?line=210'>211</a>\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './saved/hffh_ViT_v1.tar'"
     ]
    }
   ],
   "source": [
    "s = Saver()\n",
    "args, m, l, t = s.Load(d, HFFH_ViT, Loss, Trainer, \"./saved/hffh_ViT_v1.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c3cd6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01f572d74fdf3d74e608f44a32fff06de9c7e8ec7cec3cccff4f4ed186f21499"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
